{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT3YtrdwFlxn",
        "colab_type": "text"
      },
      "source": [
        "#Using BERT architecture to measure ESG qualities of a tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_tHhSPg2soo",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2rm4932bUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/google-research/bert.git\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os \n",
        "import re\n",
        "import numpy as np\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Lambda\n",
        "import math\n",
        "import io\n",
        "from matplotlib import pyplot\n",
        "import copy\n",
        "\n",
        "sess = tf.Session() #initializes a session\n",
        "\n",
        "#set some parameters for model and tokens\n",
        "bert_path = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
        "max_seq_length = 256\n",
        "n_classes = 13"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TF_Jj-r3tpp",
        "colab_type": "text"
      },
      "source": [
        "###Import Data from Local Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzSw-GNQ3w4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Have 13 seperate targets\n",
        "from google.colab import files\n",
        "tweets = files.upload()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WCuIW77YgQ9",
        "colab_type": "text"
      },
      "source": [
        "###Load Data\n",
        "(This will get rid of neutral polarities.)\n",
        "\n",
        "(Do not use this one for now it doesn't work with multiple targets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mspx0UzgQKLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import random\n",
        "#Try to load with pandas\n",
        "df = pd.read_csv(io.BytesIO(tweets['Twitter_Data.csv']))\n",
        "typeList = ['Business Ethics - S','Business Ethics - G', 'Anti-Competitive Practice', 'Corruption & Instability',\n",
        "           'Privacy & Data Security', 'Discrimination','Toxic Emissions & Waste',  'Health & Demographic Risk',\n",
        "           'Supply Chain Labour Standards or Labour Management', 'Carbon Emissions', 'Product Quality & Safety',\n",
        "           'Polarity', 'Related To Company or Not'] #Thirteen classes\n",
        "\n",
        "df = df.drop(columns = {'Student','company','ESG','keyword','keyword mapping'})\n",
        "df = np.array(df).tolist() #for each element of df 0 is content, 1-10 are labels, second to last is polarity, last is related to company \n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "train_text = []\n",
        "train_label = []\n",
        "test_text = []\n",
        "test_label = []\n",
        "\n",
        "for twt in df:\n",
        "  notempty = False\n",
        "  for i in range(1,n_classes+1):\n",
        "    if math.isnan(twt[i]):\n",
        "      twt[i] = 0\n",
        "    else:\n",
        "      notempty = True\n",
        "  twt[12] = ( twt[12] + 1 )/2\n",
        "\n",
        "  \n",
        "  if random.random()<0.8:\n",
        "    train_data += [twt]\n",
        "  else:       #add notempty==True to remove empty ones\n",
        "    test_data +=[twt]\n",
        "\n",
        "#Whats the issue with binary\n",
        "for tweet in train_data:\n",
        "  if not tweet[0]==None:\n",
        "    train_text += [tweet[0]]\n",
        "    train_label += [[tweet[1:n_classes+1]]]\n",
        "\n",
        "    \n",
        "for tweet in test_data:\n",
        "  if not tweet[0]==None:\n",
        "    test_text += [tweet[0]]\n",
        "    test_label += [(tweet[1:n_classes+1])] #Temp measure, should be 1:13"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aOBA_3iQLak",
        "colab_type": "text"
      },
      "source": [
        "###Get Info About the Data Set\n",
        "(This function counts the number of tweets for each controversy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3em9BJG_zYhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def listtypes(train_label,test_label):\n",
        "  traincount = []\n",
        "  testcount = []\n",
        "  for i in range(0,n_classes+2):\n",
        "    traincount += [0]\n",
        "    testcount += [0]\n",
        "  \n",
        "  for tnlabel in train_label:\n",
        "    for k in range(0,n_classes):\n",
        "      if tnlabel[0][k]==1:\n",
        "        traincount[k]+=1\n",
        "      if k==11 and tnlabel[0][k]==0:\n",
        "        traincount[n_classes] += 1\n",
        "      if k==11 and tnlabel[0][k]==0.5:\n",
        "        traincount[n_classes+1] += 1\n",
        "        \n",
        "  \n",
        "  for tstlabel in test_label:\n",
        "    for k in range(0,n_classes):\n",
        "      if tstlabel[k]==1:\n",
        "        testcount[k]+=1\n",
        "      if k==11 and tstlabel[k]==0:\n",
        "        testcount[n_classes] +=1\n",
        "      if k==11 and tstlabel[k]==0.5:\n",
        "        testcount[n_classes+1] += 1\n",
        "  \n",
        "  print(\"For Training Data:\")\n",
        "  for i in range(0,11):\n",
        "    print(\"For controversy type: \" + typeList[i] + ' There are ' + str(traincount[i]) + ' tweets.')\n",
        "  print(\"Number of tweets with positive polarity: \" + str(traincount[11]))\n",
        "  print('Number of tweets with negative polarity: ' + str(traincount[13]))\n",
        "  print('Number of tweets with neutral polarity: ' + str(traincount[14]))\n",
        "  print('Number of tweets that are related to company: ' + str(traincount[12]))\n",
        "  \n",
        "  print(\"\\nFor Testing Data:\")\n",
        "  for i in range(0,10):\n",
        "    print(\"For controversy type: \" + typeList[i] + ' There are ' + str(testcount[i]) + ' tweets.')\n",
        "  print(\"Number of tweets with positive polarity: \" + str(testcount[11]))\n",
        "  print('Number of tweets with negative polarity: ' + str(testcount[13]))\n",
        "  print('Number of tweets with neutral polarity: ' + str(testcount[14]))\n",
        "  print('Number of tweets that are related to company: ' + str(testcount[12]))\n",
        "  return traincount,testcount\n",
        "\n",
        "_ = listtypes(train_label,test_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCjpAMeL3wg6",
        "colab_type": "text"
      },
      "source": [
        "###Tokenizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXxV7ohZ39s3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Seperate the data into words\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, n_classes),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        text = str(text)\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \" + text, text_b=None, label=label) # Used to be text_a = \" \".join(text) not sure if this option still works\n",
        "        )\n",
        "    return InputExamples\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_text, train_label)\n",
        "test_examples = convert_text_to_examples(test_text, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWWPfB0Y394o",
        "colab_type": "text"
      },
      "source": [
        "###Defining the BERT class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AY8eynY3-Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"mean\",\n",
        "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "        if self.pooling not in [\"first\", \"mean\"]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "        if self.pooling == \"first\":\n",
        "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "            trainable_layers = [\"pooler/dense\"]\n",
        "\n",
        "        elif self.pooling == \"mean\":\n",
        "            trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "            trainable_layers = []\n",
        "        else:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        if self.pooling == \"first\":\n",
        "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"pooled_output\"\n",
        "            ]\n",
        "        elif self.pooling == \"mean\":\n",
        "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result, input_mask)\n",
        "        else:\n",
        "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePGp4Crf3-oS",
        "colab_type": "text"
      },
      "source": [
        "###Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hci8qKv83-1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    extradense = tf.keras.layers.Dense(64,activation = 'relu')(dense)\n",
        "    pred = tf.keras.layers.Dense(n_classes, activation='sigmoid')(extradense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  #Fix this\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkJLGXM8BCad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(max_seq_length)\n",
        "\n",
        "initialize_vars(sess)\n",
        "\n",
        "#Add in early stopping\n",
        "es = EarlyStopping(monitor = 'val_loss')\n",
        "cb_list=[es]\n",
        "\n",
        "ESGmodelhistory = model.fit(\n",
        "  [train_input_ids, train_input_masks, train_segment_ids],\n",
        "  train_labels,\n",
        "  validation_data = ([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
        "  epochs = 5,\n",
        "  batch_size = 32,\n",
        "  callbacks = cb_list\n",
        ")\n",
        "\n",
        "pyplot.plot(ESGmodelhistory.history['loss'], label='train')\n",
        "pyplot.plot(ESGmodelhistory.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7FJ8iAMKGni",
        "colab_type": "text"
      },
      "source": [
        "###Predict and Evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXar40Nj4Roi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict([test_input_ids, test_input_masks, test_segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4Lg2wAFXoQ",
        "colab_type": "text"
      },
      "source": [
        "####Find Precision and Recall for a Single Threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhoI_-hZKGXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for prediction in pred:\n",
        "  for i in range(0,n_classes):\n",
        "    if prediction[i] > 0.1 and i != n_classes-2: #n_classes-2 is the polarity label\n",
        "      prediction[i] = 1\n",
        "    elif prediction[i] > 0.33 and i == n_classes-2:\n",
        "      prediction[i] = 1\n",
        "    else:\n",
        "      prediction[i] = 0\n",
        "\n",
        "measures = [] #this will hode tp,fp,tn,fn values\n",
        "for i in range(0,n_classes):\n",
        "  measures += [[0,0,0,0]]  #[tp,fp,tn,fn] for each class\n",
        "      \n",
        "for prediction,label in zip(pred,test_label):\n",
        "  for i in range(0,n_classes):\n",
        "    if prediction[i] == 1 and label[i] == 1:\n",
        "      measures[i][0] += 1\n",
        "      \n",
        "    if prediction[i] == 1 and label[i] == 0:\n",
        "      measures[i][1] += 1\n",
        "      \n",
        "    if prediction[i] == 0 and label[i] == 0:\n",
        "      measures[i][2] += 1\n",
        "      \n",
        "    if prediction[i] == 0 and label[i] == 1:\n",
        "      measures[i][3] += 1\n",
        "\n",
        "precisions = []\n",
        "recalls = []\n",
        "accuracies = []\n",
        "\n",
        "for measurement in measures:\n",
        "  precisions += [(measurement[0]/max(1,(measurement[0] + measurement[1])))] #tp/(tp+fp)\n",
        "  recalls += [(measurement[0]/max(1,(measurement[0] + measurement[3])))]\n",
        "  accuracies += [(measurement[0]+measurement[2])/len(pred)]\n",
        "print(precisions)\n",
        "print(recalls)\n",
        "print(accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikNdLJEFb0c",
        "colab_type": "text"
      },
      "source": [
        "####Create ROC curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_E4kPa1ErL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pred[30][4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQA7KVMZFfYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genROC(pred, labels, n, min_thresh, max_thresh, classid, n_classes): #This returns the tpr and fpr for n number of threshold values. !!It does not display the graph\n",
        "  if classid > n_classes-1 or n<0:\n",
        "    print('ERROR: classid is out of range')\n",
        "    return False, False\n",
        "  \n",
        "  fprlist = [] #x-axis values\n",
        "  tprlist = [] #y-axis values\n",
        "  \n",
        "  #Start by setting changing the curve based on the threshold\n",
        "  for thresh in range(0,n):\n",
        "    thresh = (max_thresh - min_thresh) * float(thresh)/n\n",
        "    thresh = thresh + min_thresh\n",
        "    newpred = copy.deepcopy(pred)\n",
        "    for p in newpred:\n",
        "      if p[classid] > thresh:\n",
        "        p[classid] = 1\n",
        "      else:\n",
        "        p[classid] = 0\n",
        "    \n",
        "    measures = [0,0,0,0] #[tp,fp,tn,fn]\n",
        "    \n",
        "    for p,label in zip(newpred,test_label):\n",
        "      if p[classid] == 1 and label[classid] == 1:\n",
        "        measures[0] += 1\n",
        "      \n",
        "      if p[classid] == 1 and label[classid] == 0:\n",
        "        measures[1] += 1\n",
        "      \n",
        "      if p[classid] == 0 and label[classid] == 0:\n",
        "        measures[2] += 1\n",
        "      \n",
        "      if p[classid] == 0 and label[classid] == 1:\n",
        "        measures[3] += 1\n",
        "        \n",
        "    tpr = float(measures[0]) / (measures[0] + measures[3])  #tp/(tp+fn)\n",
        "    fpr = float(measures[1]) / (measures[1] + measures[2])  #fp/(fp+tn)\n",
        "    \n",
        "    \n",
        "    \n",
        "    fprlist += [fpr]\n",
        "    tprlist += [tpr]\n",
        "    \n",
        "  return fprlist,tprlist\n",
        "    \n",
        "fpr,tpr = genROC(pred,test_label, 2000, 0.0308, 0.0309, 4, n_classes)\n",
        "\n",
        "pyplot.plot(fpr,tpr)\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.title('ROC curve')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-P96MdvFfpH",
        "colab_type": "text"
      },
      "source": [
        "####Create PR curves to find AUPRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFbw829ZI3Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genPRC(pred, labels, n, min_thresh, max_thresh, classid, n_classes): #This returns the tpr and fpr for n number of threshold values. !!It does not display the graph\n",
        "  if classid > n_classes-1 or n<0:\n",
        "    print('ERROR: classid is out of range')\n",
        "    return False, False\n",
        "  \n",
        "  fprlist = [] #x-axis values\n",
        "  tprlist = [] #y-axis values\n",
        "  bestthresh = 0 #Will choose the best threshold based on F1 score\n",
        "  bestf1 = 0\n",
        "  \n",
        "  #Start by setting changing the curve based on the threshold\n",
        "  for thresh in range(0,n):\n",
        "    thresh = (max_thresh - min_thresh) * float(thresh)/n\n",
        "    thresh = thresh + min_thresh\n",
        "    newpred = copy.deepcopy(pred)\n",
        "    for p in newpred:\n",
        "      if p[classid] > thresh:\n",
        "        p[classid] = 1\n",
        "      else:\n",
        "        p[classid] = 0\n",
        "    \n",
        "    measures = [0,0,0,0] #[tp,fp,tn,fn]\n",
        "    \n",
        "    for p,label in zip(newpred,test_label):\n",
        "      if p[classid] == 1 and label[classid] == 1:\n",
        "        measures[0] += 1\n",
        "      \n",
        "      if p[classid] == 1 and label[classid] == 0:\n",
        "        measures[1] += 1\n",
        "      \n",
        "      if p[classid] == 0 and label[classid] == 0:\n",
        "        measures[2] += 1\n",
        "      \n",
        "      if p[classid] == 0 and label[classid] == 1:\n",
        "        measures[3] += 1\n",
        "        \n",
        "    precision = float(measures[0]) / (measures[0] + measures[3])  #tp/(tp+fn)\n",
        "    fpr = float(measures[1]) / (measures[1] + measures[2])  #fp/(fp+tn)\n",
        "    \n",
        "    \n",
        "    \n",
        "    fprlist += [fpr]\n",
        "    tprlist += [tpr]\n",
        "    \n",
        "    if f1>bestf1:\n",
        "      bestthresh = thresh\n",
        "      bestf1 = f1\n",
        "    \n",
        "  return fprlist,tprlist,bestthresh\n",
        "    \n",
        "fpr,tpr = genROC(pred,test_label, 2000, 0.0308, 0.0309, 4, n_classes)\n",
        "\n",
        "pyplot.plot(fpr,tpr)\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.title('ROC curve')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}